---
layout: post-no-feature
title: Predicting expected goals for NHL teams
description: "In this article we discuss about modelling and experiment tracking using comet.ml "
categories: articles
tags: [sample post, images, test]
---
## Experiment Tracking
come.ml is used for tracking model and its evaluation metrics.
## Feature Engineering I
**1)**
Distance from net is calculated as L2 norm of difference between normalized x,y coordinates and normalized (89,0) goal coordinates.
From the below **histogram of shot counts for goals vs Distance from net** it can be observed that there are more number of shot counts for bin 9.36 to 18.73 distance and number of count decreases after this bin as distance from net increases.

<img src="/images/plot_goal1-1.png" alt="">

There is similar observation for below **histogram of shot counts for No goals vs distance from net** where number of shot counts for bin 9.48 to 18.96 distance is highest compared to other bins.

<img src="/images/plot_NOGOAL_1-1.png" alt="">

Comparing both **histogram for shot counts for goals and no goals** there has been less number of shot counts for goals compared to no goals which means that between distance 0 to 66.54 there has been more shots counts but very few lands up being goal. And there has been significant less number of shots after distance 66.54 in both the graphs.

Angle from net: Angle between (x, y) coordinates, and the goal middle line (x, 0). When facing the goal, positive values represent the right side, and negative values the left side. Values between [0, 90] and [-90, 0] indicate the front of the goal, and those between [90, 180] and [-90, -180] behind the goal.(Also given in Q4.)
<img src="/images/plot_goal1-2.png" alt="">
<img src="/images/plot_Nogoal1-2.png" alt="">
From above two histogram of shot counts for goals and no goal, binned by angle, there has been more number shot(goal or no goal) in angle for range -38.11 to 40.38 and in angle between -7.86 to 9.21 has more shots which resulted in goal.
A 2D histogram where one axis is the distance and the other is the angle.
<p align="center">
    <img src="/images/jointplot.png" alt="">
</p>
From above jointplot, more data points varies for angle from net when distance from net is very less and data points for angle from net decreases as distance from net increases which mean that there has been less number of shot from large distance.
From geometrical perspective we can understand that at higher distance from net it is less likely to make a goal from varying angle because more opponents stopping the shot.<br>

**2)** From below Goal rate vs Distance from net plot, Goal rate is higher for less distance from net which means there is high chance of being a shot goal if its distance from net is less.
<img src="/images/Q1-4.png" alt="">
From below Goal rate vs angle from net plot, there is high goal rate observed for higher angle(90 to 180) from net which is from behind the goal. This is strange. Since this is the goal rate, it is not indicative of the frequency of shots at ~170 degrees. Shots at 179 are probably plays like this
<a href="https://youtu.be/Mxfa0bmiQ2g?t=39" target="_top">https://youtu.be/Mxfa0bmiQ2g?t=39</a>. When such shots are taken, they have a high chance of success.

<img src="/images/Q1-5.png" alt="">


**3)**
Since it is rare to score a non-empty net goal on the opposing team from within your defensive zone, there are less shot counts for higher distance and no shot counts for goal for distance from net greater than 84.23 as shown in below two graph.<br>
Histogram of goals only, binned by distance, and non-empty net events.
<img src="/images/empty_net0_dist.png" alt="">
Histogram of goals only, binned by angle, and non-empty net events.
<img src="/images/empty_net0_angle.png" alt="">

In below for Histogram of goals only, binned by distance, and empty net events there are events at higher distance for empty but it is not an anomaly
<img src="/images/empty_net1_dist.png" alt="">
Histogram of goals only, binned by angle, and empty net events.
<img src="/images/empty_net1_angle.png" alt="">
We don't suspect any anomaly for the data we considered for regular season from 2015/16 - 2018/19.

## Baseline Models
**1)** After spliting data(2015-2019) into train and test set in 80:20 ratio we trained logistic regression(no hyperparameters were tuned) which gave below classification report where overall accuracy is 90%. Since there is high imbalance in data and only single feature - Distance from net is used, the F1 score for number of 1s(i.e target value as GOAL) is 0 where as F1 score for 0 (target value as No GOAL) is 95%.
Below results are on validation data considering 'Distance from net' as feature
<img src="/images/logistic_baseline.png" alt="" width = "500">

**3)**Comparison of Logistic Regression with different set of features
ROC_AUC score. It can be observed that logistic regression for features considering Distance and angle from net AUC score is highest compared to others. For logistic regression considering Distance feature only and logistic regression considering angle feature only has 0.50 AUC score which is less than taking both the features for model building. It can be including more features will yield better prediction results.  
<img src="/images/ROC_AUC_comparison.png" alt="">

Below Graph is for Goal rate (#goals / (#no_goals + #goals)) as a function of the shot probability model percentile. It is observed that Logistic regression build using 2 features(distance and angle from net) has increase in Goal rate when shot probability model percentile increases. Also, Logistic regression considering distance feature only overlaps with Logistic regression built using 2 features which interprets that Distance from net feature will help improve prediction as GOAL.
<img src="/images/Goal_rate_comparison.png" alt="">
Below Graph is for cumulative proportion of goals as a function of the shot probability model percentile. Blue and green line overlaps and its cumulative % of Goal increases as Shot probability model percentile decreases.    
<img src="/images/cum_Shot_prob_comparison.png" alt="">
The reliability diagram (calibration curve) represents how well calibrated the predicted probabilities are using calibration curves. From below diagram, the curve(green color) for logistic regression built using Distance and angle from net feature has increase in fraction of positives as Mean predicted probability increases where as for Random baseline(Red color) it remains constant. For logistic regression built using only angle from net feature and Logistic regression built using Distance from net feature only does not have increase in fraction of postive as mean predicted probability increases.
<img src="/images/Calibration_comparison.png" alt="">

 **4)**Link to evaluation metric experiment:
 <a href="https://www.comet.ml/zilto/hockey-all-star-analytics/b0a87f672f26431b878324a12ecedcfe" target="_top">https://www.comet.ml/zilto/hockey-all-star-analytics/b0a87f672f26431b878324a12ecedcfe</a>

Link to model registry:
<a href="https://www.comet.ml/zilto/hockey-all-star-analytics/56d1863b2ddf4fafa0f43fff65b74e53" target="_top">https://www.comet.ml/zilto/hockey-all-star-analytics/56d1863b2ddf4fafa0f43fff65b74e53</a>

# Feature engineering II

The NHL data contains rich information that allows complex features to be engineered. Here's a list of features implemented with a short description:

1. **seconds_elapsed**: Total number of seconds of game time. Takes the current period time in seconds + 1200 seconds per regular period completed + 300 or 1200 seconds per overtime period completed (to account for playoff vs. regular season games)
2. **game_period_idx**: Index of the current period, starting at 1. Period index above 3 indicate overtime.
3. **x_coord**: X coordinate on the ice rink in foot. Ranges from -100 to 100
4. **y_coord**: Y coordinate on the ice rink in foot. Range from -43 to 43
3. **x_coord_norm**: Change the sign of the X coordinate to have all offensive plays on the positive range of x-values
4. **y_coord_norm**: Change the sign of the Y coordinate to maintain the relative position to the net. This ensure correct handedness of shots.
5. **dist_from_net**: Distance between the normalized (x, y)  coordinates of the shot, and the normalized goal at (89, 0). It is the L2 norm of the vector between the two points.
6. **angle_from_net**: Angle between (x, y) coordinates, and the goal middle line (x, 0). When facing the goal, positive values represent the right side, and negative values the left side. Values between [0, 90] and [-90, 0] indicate the front of the goal, and those between [90, 180] and [-90, -180] behind the goal.
7. **shot_type**: Shot type of the current event. The categories were one-hot encoded (Backhand, Slap shot, Snap Shot, Tip-In, Wrap-around, Wrist Shot, Deflected).
8. **empty_net**: Boolean indicating if the net was empty at the time of the shot. The data is missing for the majority of examples. The data field included information about Powerplay, and Shorthanded goals, leading to data leakage in the model (i.e., it is correlated with the label).
9. **previous_event_type**: The type of the previous recorded event. Masked to assure the previous event happened during the same game period. The categories were one-hot encoded (BLOCKED_SHOT, FACEOFF, GIVEAWAY, HIT, MISSED_SHOT, SHOT, TAKEAWAY).
10. **previous_x_coord**: X coordinate of the previous event. Masked to assure the previous event happened during the same game period.
11. **previous_y_coord**: Y coordinate of the previous event. Masked to assure the previous event happened during the same game period.
12. **seconds_from_previous**: Total number of seconds of  game time since last event. Masked to assure the previous event happened during the same game period.
13. **dist_from_previous**: Distance between (x, y) coordinates of the current and previous events. Masked to assure the previous event happened during the same game period.
14. **rebound**: Boolean indicating if the shot was a rebound. A rebound happens when the previous event was also a shot by the same team. Masked to assure the previous event happened during the same game period.
15. **angle_change**: If the current event is a rebound, indicates the difference of angle between the two shots; otherwise 0.
16. **speed**: Distance from the previous event divided by the seconds from previous event. It should be noted that Faceoffs, or penalty shots involve stopping the game time, and repositioning players. This can give the impression of large distances being crossed in a short amount of time.

The DataFrame for the game between Winnipeg and Washington on March 12, 2018 (gamePk = 2017021065) can be viewed at the following url:
<a href="https://www.comet.ml/zilto/hockey-all-star-analytics/dc62512990544ff894f98a6ec274f72e" target="_top">https://www.comet.ml/zilto/hockey-all-star-analytics/dc62512990544ff894f98a6ec274f72e</a>

## Advanced Models

**1)** First, a baseline was established by training an XGBoost classifier with only the features *distance* from the net and *angle* from the net (*XGBoost base*). This XGBoost baseline model exhibited a ROC area under the curve of 0.72, which is an improvement over the logistic regression baseline. The calibration curve suggests the model has difficult formulating appropriate predictions, by under evaluating between 0.2 and ~0.7, and then overshooting.

Similarly to the logistic regression models trained, a 80:20 split was used for training and validation. The classifier was trained with default hyperparameter values of 100 estimators, a max tree depth of 4, and a learning rate of 1. No hyperparameter tuning was conducted.
<table>
<tr>
<td valign="top"><img src="/images/Q5/base_roc.PNG" width="300" ></td>
<td valign="top"><img src="/images/Q5/base_rate.PNG" width="300" ></td>
</tr>
<tr>
<td valign="top"><img src="/images/Q5/base_cumulative.PNG" width="300" ></td>
<td valign="top"><img src="/images/Q5/base_calibration.PNG" width="300" ></td>
</tr>
</table>
(Link to experiment:<a href="https://www.comet.ml/zilto/hockey-all-star-analytics/8096ba562d24434f845b602ba0cf830d" target="_top">https://www.comet.ml/zilto/hockey-all-star-analytics/8096ba562d24434f845b602ba0cf830d</a>)

**2)** Then, another XGBoost classifier was trained using the advanced features developed, and was tuned for best performance (*XGBoost tuned*). This model has an ROC AUC of 0.76, which is an improvement over the XGBoost base. More importantly, the XGBoost tuned seem way better calibrated, with some overshoot after ~0.7.

XGBoost models have a large amount of hyperparameters to modify. For the purpose of this task, the search space was defined over the following features: the max tree depth (*max_depth*), the learning rate, the number of estimators, the L1 regularization, the L2 regularization, and the minimum weight of a node before splitting (*min_child_weight*). They were selected because they play an important role in the model's ability to learn, and to generalize.

The optimization was completed with the library *optuna*, which allows to define an *objective* that needs to be optimize by iterating through hyperparameters over several *trials*. The objective was to minimize the average of "best logloss" of a model with a set of hyperparameters trained 5 times over different cross-validation splits. Different hyperparameter configurations were tested over 20 trials.
<table>
<tr>
  <td valign="top"><img src="/images/Q5/tuned_roc.PNG" alt="" width="300"></td>
  <td valign="top"><img src="/images/Q5/tuned_rate.PNG" alt="" width="300"></td>
  </tr>
  <tr>
  <td valign="top"><img src="/images/Q5/tuned_cumulative.PNG" alt="" width="300"></td>
  <td valign="top"><img src="/images/Q5/tuned_calibration.PNG" alt="" width="300"></td>
  </tr>
  </table>
(Link to experiment:
<a href="https://www.comet.ml/zilto/hockey-all-star-analytics/adf2a42dc28b40f689c770fd9c9e631d" target="_top">https://www.comet.ml/zilto/hockey-all-star-analytics/adf2a42dc28b40f689c770fd9c9e631d</a>

("best-xgb" in model registry)

**3)** Finally, the feature importance of the XGBoost tuned model was inspected with the goal of reducing the feature set, and training another classifier (*XGBoost reduced*). The overall performance is almost identical to the *XGBoost tuned*, with the same ROC AUC of 0.76.

Using the XGBoost package, the feature importance was defined as the *gain* by splitting on a feature. Based on the plot, the following features were removed: *period_idx*, *PERIOD_START* (event_type category), *PENALTY* (event_type category), *x_coord*, and *y_coord*. The coordinates were removed because they seemed redundant with their normalized counterparts. Also, the events *BLOCKED_SHOT*, *MISSED_SHOT*, and *SHOT* were combined since they all represented "shot" events, but were not too important individually. Below is feature importance plot for *XGBoost tuned*
<p align="center">
<img src="/images/Q5/tuned_features.PNG" alt="Feature importance of tuned features" width="400">
</p>
 Also, it should be noted that the feature *empty_net* was removed from the *XGBoost tuned* and *XGBoost reduced* because it was leaking information about the labels (see below figure).
<p align="center">
<img src="/images/Q5/leak.PNG" alt="" width="400">
</p>
Once the feature subset was determined, the *XGBoost reduced* was instantiated with the best parameters of the *XGBoost tuned* model, then trained on a single train-validation split. The calibration curve suggests the *XGBoost reduced* is better calibrated with less overshoot, but drifts off slightly into undershooting. Below are the figures for *XGBoost reduced* model:
<table>
<tr>
  <td valign="top"><img src="/images/Q5/reduced_roc.PNG" alt="ROC curve" width="300"></td>
  <td valign="top"><img src="/images/Q5/reduced_rate.PNG" alt="" width="300"></td>
  </tr>
  <tr>
  <td valign="top"><img src="/images/Q5/reduced_cumulative.PNG" alt="" width="300"></td>
  <td valign="top"><img src="/images/Q5/reduced_calibration.PNG" alt="" width="300"></td>
  </tr>
  </table>
  And below is the feature importance plot for all selected features from *XGBoost reduced*
  <p align="center">
  <img src="/images/Q5/reduced_features.PNG" alt="Feature importance of selected features" width="400">
  </p>
(Link to experiment: <a href="https://www.comet.ml/zilto/hockey-all-star-analytics/c8fd976332464360a2cf29b9c90b3eb1" target="_top">https://www.comet.ml/zilto/hockey-all-star-analytics/c8fd976332464360a2cf29b9c90b3eb1</a>)

("reduced-xgb" in model registry)

## Give it your best shot!

**1)** We performed feature selection using Recursive feature elimination technique(using sklearn) using logistic regression and Random forest and individually selected 30 features. The common features from both these method were used for building models. We also tried Information value but the score was very low for most of the features.

We tried Random forest, MLP classifier and Lightgbm models for this part. Hyperparameter tuning was performed using RandomizedSearchCv for 5 cross validation set on all the models individually. And for the best parameter, models were trained on 70% of data and validated on 30% of data. Since data is highly imbalanced(# of no goal: 90.6% # of goals: 9.38%)

For Lightgbm and Random forest we set parameter 'classweight' as {0: 0.2, 1: 0.8} which means we assign more weight to number of 1's. Other parameters used for Random forest for tuning was max_depth(=6 as best parameter) and n_estimator(=500 as best parameter).

For Multilayer perceptron, parameters such as hidden layer size, regularizer(alpha) and activation was tuned but gave 0 predictions on validation set, hence it performed better without tuning parameters and max_iter=300.

For lightgbm model, regularization was added using reg_alpha(=0.4 as best parameter) which is L1 regularizer and reg_lambda(0.7 as best parameter) which is L2 regularizer. Metric used was AUC score for Lightgbm model. We also used F1 score as a metric for individual target value for all the models, due too this we were able to increase predictions on number of Goals.

Metric comparison for all models:
<p align="center">
  <img src="/images/Q6_metric_compare.png" alt="" width="1000">
</p>
**Best performing model was lightgbm with AUC score as 0.768 and F1 score as 0.32 on target value as GOAL**.
In Below curve for ROC almost overlaps with all the three model and AUC score of LightGBM model is highest.
<p align="center">
  <img src="/images/ROC_AUC_Q6.png" alt="">
</p>
Similar observation is for below two graphs where all three curves overlap and might lead to better prediction results since Goal rate increases as Shot probability model percentile increases(on left graph).
<table>
<tr>
  <td valign="top"><img src="/images/Goal_rate_Q6.png" alt="" ></td>
  <td valign="top"><img src="/images/cum_Shot_prob_Q6.png" alt=""></td>
  </tr>
  </table>
Observing below reliability diagram and comparing all three model curves LightGBM has less spikes than compared to MLP and its Fraction of positives increases as mean predicted probability increases. Whereas for Random forest there is no fraction of positives found after 0.8 mean predicted probability.
<p align="center">
  <img src="/images/Calibration_Q6.png" alt="">
</p>

**2)** Link to metric:
<a href="https://www.comet.ml/zilto/hockey-all-star-analytics/e1befcdc7ea2498e83f7fa40c58b2512" target="_top">https://www.comet.ml/zilto/hockey-all-star-analytics/e1befcdc7ea2498e83f7fa40c58b2512</a>
Link to experiment:
<a href="https://www.comet.ml/zilto/hockey-all-star-analytics/2becb8381b92472b82dadb51251a75f0" target="_top">https://www.comet.ml/zilto/hockey-all-star-analytics/2becb8381b92472b82dadb51251a75f0</a>
(Light gbm model is registered as "lightgbm-best-model", Multilayer perceptron as "mlp-classifier" and Random forest as "random-forest" in model registry)

## Evaluate on the test set

After training and validating several models, it is now time to assess their generalizability on independent test sets. We will consider: the logistic regression with distance (*LogReg: dist*), the logistic regression with angle (*LogReg: angle*), the logistic regression with distance and angle (*LogReg: dist & angle*), the reduced XGBoost model (*XGBoost reduced*), and the LightGBM model (*LGBM*). Each trained model will be evaluated on the games from the regular season of 2019-2020, and the games from the playoffs of 2019-2020.

### Regular games 2019-2020
First observation, the *LogReg: dist*, and *LogReg: dist & angle* almost completly overlay in each plot. Since *LogReg: angle* makes almost random prediction (AUC=0.51), it suggests that *LogReg: dist & angle* almost exclusively relies on the distance information, which also appeared as the most important feature in XGBoost models.

Additionally, the XGBoost and LGBM also display almost identical metrics, which is consistent with the fact that they both are gradient boosting machines. However, the calibration curve suggests a better calibration for the *XGBoost reduced*, close to the one observed during its training. This suggests that the model generalized well to this test set. This could be the result of the reduced set of features, limiting the influencing of "irrelevant" or "noisy" features.
On Evaluation metrics on Regular test data
<table>
<tr>
  <td valign="top"><img src="/images/roc_q7_regular.PNG" alt="" width="400"></td>
    <td valign="top"><img src="/images/goal_rate_regular_q7.PNG" alt="" width="400"></td>
    </tr>
    <tr>
    <td valign="top"><img src="/images/cumulative_regular_q7.PNG" alt="" width="400"></td>
    <td valign="top"><img src="/images/calibration_regular_q7.PNG" alt="" width="400"></td>
</tr>
</table>
### Playsoffs 2019-2020
The main distinction between the Regular season and Playoffs test set is the performance of the XGBoost model. Indeed, *XGBoost reduced*'s performed worst on all indicators, even falling below the *LogReg* models in terms of ROC AUC. A first hypothesis is that the advanced features are too noisy to be helpful, especially in a small test set where the variance is increased. An alternative hypothesis would be that playoffs games are qualitatively different from regular game, such that the learning from the *XGBoost reduced* did not generalized.

An interesting source of insight is the sustained performance of the *LGBM* model. Since XGBoost and LGBM are both members of the same family, what lead to their disparity in performance? It would be surprising that it truly is the feature set that distinguishes because only the least relevant features were dropped. This only raises more questions to investigate, in particular, are there differences in robusteness between the algorithms, or regularization within their hyperparameters.

On Evaluation metrics on Playoffs test data
<table>
<tr>
<td valign="top"><img src="/images/roc_poff.PNG" alt=""  width="400"></td>
<td valign="top"><img src="/images/rate_poff.PNG" alt="" width="400"></td>
</tr>
<tr>
<td valign="top"><img src="/images/cumulative_poff.PNG" alt="" width="400"></td>
<td valign="top"><img src="/images/calibration_poff.PNG" alt="" width="400"></td>
</tr>
</table>
